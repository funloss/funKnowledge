# 1. 语义表达
## 词向量如何建模语义信息?稀疏词向量和稠密词向量有什么区别?
1. 词向量将自然语言映射到向量空间， embedding
2. 稀疏词向量：one-hot， 问题，彼此正交，距离的远近无法作为语义相似度的度量
3. 分布式语义假设：对人类语言的归纳偏置，『一个词的含义与其上下文具有很强的相关性』
4. 稠密词向量：融入先验知识将语义信息嵌入到低维连续空间向量中，先验知识的引入使得词语的表征具备语义聚类的特性
5. word2Vec，是一种浅层神经网络：
	1. 包括skip-gram和CBOW两种模型
	2. 提升模型训练速度的两种方法：层次化softmax，负采样
	3. 与bert的主要区别：无法解决一词多义的问题，bert的多头注意力可以捕捉到不同的信息

## 在构建词向量的过程中，怎么处理溢出词表词问题?
1. OOV（out of vocabulary），影响：模型在下游任务性能变差，弱化了模型泛化能力，限制了模型应用范围
2. 解法：
	1. 丢字母，改写，提取词干等查找
	2. 构建向量力度更小的子词向量
	3. 字符级别的n-gram向量（FastText）

## 词/子词/字符粒度的分词方法对构建词向量有何影响?
4. 词，英文的空格分割，不如子词稠密（do，doing完全不同，冷门词学习不充分）
5. 子词，粒度可以控制词根，词缀等，可以捕捉单词的内部结构（词性，时态，语法功能
	1. wordPiece，词片分词；BPE，字节对编码：都是将小的字符组成的基本词表不断扩充为短词组成的词元列表
	2.  对于子词合并的策略，BPE是不断和并出现频率最高的相邻子词，wordPiece执行合并时目标是最大化语料库的似然值
6. 字符，char，可以处理任何语言，但是信息过于细微，中文还好，对于英文单独的字母没有太多的意义

## 如何利用词向量进行无监督句子相似度计算任务?
1. 直接词向量加权平均作为句子向量，快速简单，但是不考虑重要性和语序，长文本效果也会很差
2. 词向量使用tf-idf作为权重来加权计算
3. 词移距离（Word Mover Distance），最小代价最大流(MCMF)求解，复杂度 $O(p^3 log(p))$

## 如何使用 BERT 构建有聚类性质的句子向量?
1. 句子向量的表征结果，需要考虑两个方面： 对齐性（alignment）语义相近的句子应该尽可能相近；均匀性（uniformity），语义不相近的句子的表征在超空间中分布均匀
2. 解法：引入对比学习来增强BERT的语义感知能力
3. 代表模型：Sentence-Bert， SimCSE

## 基于 Transformer 的预训练语言模型如何区分文本位置?
1. 引入位置编码
2. 绝对位置编码，transformer使用的sinusoidal
3. 相对位置编码，RoPE ALiBi等

## 为什么在 BERT 的输人层中3种嵌入表达要相加?
1. BERT中，词元嵌入，位置嵌入，分段嵌入相加，是一种特征交叉的做法
2. 对比concat，可以保持结构的一致性，可以提升性能，也是强制的显性特征交叉
3. 有工作指出，位置和词元的语义关联并不强，可能拆出去效果更好

## 大模型的隐含语义是如何建模的?有哪几种典型架构?
1. 大模型通过零样本推断的能力，证明了大模型可以直接将隐含的语义直接建模到完整的基于Transformer的语言模型内部，并不需要显式的二次下游的微调
2. 典型架构：纯编码器（生成能力弱），编码器-解码器架构（编码双向，解码单向），纯解码器架构（单向注意力，使用最多）

# 2. 大模型的数据
## 用来训练大模型的开源数据集有哪些?
1. 维基百科，图书，学术论文，网络爬虫，代码

## 主流开源大模型所用的训练数据量如何?各个环节的数据量如何?
略，关注各个模型的技术报告。qwen3 使用了36t token的数据

## 大模型数据预处理流程要注意哪些核心要点?
数据的质量，典型的数据清洗方法
数据的多样性，使用额外的模型通过聚类或者监督的方式来控制和评价数据的多样性

## 大模型中的扩展法则是什么?如何推演?
1. 扩展法则（scaling law）：模型的性能提升与模型的参数量大小N，数据集大小D和训练的计算量C密切相关
2. 对于常见的纯解码器模型，基本满足 $C \approx 6ND$

## 持续预训练有什么作用?如何缓解大模型微调后的通用能力遗忘问题?
1. 持续预训练的目的是向模型注入一些领域相关知识，对训练数据的质量要求比较高
2. 灾难性遗忘：模型更加倾向于为了拟合微调数据的分布情况而提升领域内相关说法的概率，导致通用数据训练后的概率空间被破坏，形成了一个有偏的模型

## 大模型指令微调有哪些筛选数据的方法?
1. 关键挑战：如何从大量数据中选择最有助于提升模型性能的训练数据
2. 除了人工筛选，当前的自动化筛选方法大多关注数据的质量，多样性，必要性这三大特点去进行选择
3. Nuggets，提出了一种方式，可以度量一条训练数据的有用程度。如果一条数据作为one-shot，对比zero-shot提升很大，则认为这条数据对于大模型的效果提升帮助很大
4. CaR，分两个阶段，数据质量打分和数据多样性排序


# 3. 大模型的预训练
大模型训练的三个阶段：**预训练，监督微调和偏好对齐**
## 预训练和监督微调有什么区别和相同之处?
1.  对于纯解码器模型，预训练的训练任务是预测下一个token（next token prediction，NTP）。其中每个token的预测仅依赖于前面的token，通过因果注意力掩码来实现（causal attention mask）。损失函数是逐token计算的
2. 监督微调同样是token的预测任务，区别是监督微调需要回传段落级别的损失
## 大模型的涌现能力指的是什么?
1. 如果一种能力不存在于规模较小的模型当中，但是存在于规模更大的模型当中，那么该能力就是涌现的
2. 能够验证大模型涌现能力的方法：少样本提示方法，增强提示策略（cot等）。都能说明，模型规模达到一定阈值后，模型表现突然提升
3. 这种也可能是评价指标不够平滑，尚无定论
## 大模型在预训练阶段有哪些提效实验和保障稳定性的方法?
1. 模型架构优化和训练策略调整：张量并行（TP）通信量大于流水线并行（PP），以及专家并行（EP）的配置优化
2. 根据scaling law合力分配参数量和资源
3. 中断恢复，自动化指标监控，使用实验管理工具和构建流水线等
## 大模型预训练、监督微调和强化学习分别解决什么问题?有什么必要性?
1. 预训练：将语料库的知识压缩进模型参数中，解决两个关键问题：1.如何利用大规模的无标注语料进行有效训练，2.如何构建能够捕捉隐含语义的表达和计算机制。基本认知：大模型在预训练已经有了大部分的知识储备
2. 监督微调：数据质量要求更高，往往是人工撰写和标注。模型可以更好的对齐人类希望的交互格式，风格和内容。『对齐税』，指模型可能过拟合监督微调的数据
3. 可以理解成，预训练是监督微调的冷启动，监督微调是强化学习的冷启动。强化学习能够更加精细的对齐人类的期望和偏好
## 大模型训练过程中如何计算显存?优化显存占用的方法有哪些?
1. 深度神经网络训练的显存消耗主要是两部分：1.模型权重训练占用的优化器状态，模型权重以及梯度。2. 模型各个非线性模块的激活值

设模型参数量为  $\Phi$

模型本身的显存占用和模型量化程度的关系：

| 量化程度      | 显存占用     |
| --------- | -------- |
| FP32      | $4\Phi$  |
| FP16/BF16 | $2\Phi$  |
| INT8      | $1\Phi$  |
| INT4      | $<1\Phi$ |
模型在非训练状态下的显存占用计算公式：

$$
\Phi = n_{vocab} \times d_{hidden}  + n_{layer} \times [d_{hidden} + (2 + 2 \frac{n_{kv\_head}}{n_{head}})d^2_{hidden} + d_{hidden} + 3\times d_{hidden}d_{FFN}] + d_{hidden} +  d_{hidden}\times n_{vocab} 
$$
训练过程中，考虑到AdamW优化器状态的参数量，和FP32训练，相关的显存状态为
$$M_{Model} = 4 \Phi $$
$$ M_{grad} = 4 \Phi $$
$$ M_{optim} = M_{momentum,FP32} + M_{variance,FP32}= 4 \Phi + 4 \Phi = 8 \Phi $$
$$M{total} = 16\Phi$$

开启混合精度训练后，优化器需要额外存储一份FP32精度的模型权重副本来稳定训练，参数量变为

$$M_{Model} = 2 \Phi $$
$$ M_{grad} = 2 \Phi $$
$$ M_{optim} = M_{model,FP32} + M_{momentum,FP32} + M_{variance,FP32}= 4 \Phi + 4 \Phi + 4 \Phi= 12 \Phi $$
$$M{total} = 16\Phi$$
可见，开启混合精度并没有减少显存占用，混合精度的有点是半精度计算加速了模型前向传播的速度，同时降低了中间激活值的显存占用。


激活内存主要由注意力机制和FFN产生，总激活显存的计算是：
$$
M_{total,act} = n_{layer} \times [(10+4\frac{n_{kv\_head}}{n_{head}})bsd_{hidden} + 8bsd_{FFN} + 5bs^2n_{head}] + 4bsd_{hidden}
$$

大模型并行策略主要有两个——数据并行和模型并行，模型并行又分为张量并行（大模型按照矩阵分块计算）和流水线并行（按层切分计算）

流水线并行与ZeRO-1相结合，显存消耗
$$
M_{per\_gpu} = \frac{2\Phi}{D_{tp}D_{pp}} + \frac{2\Phi}{D_{tp}D_{pp}} + \frac{12\Phi}{D_{dp}D_{tp}D_{pp}}
$$

$D_{dp}D_{tp}D_{pp}$ 就是显卡总数（分别是数据，张量和流水线的并行度），所以万卡集群中优化器状态甚至可以忽略不计
## 大模型训练过程中的通信开销如何计算?
1. 概念：显存墙，通信墙
2. 集合通信原语：Broadcast，Scatter，Gather，Reduce，All-Gather，Reduce-Scatter，All-Reduce，All-to-All
3. 张量并行下，FFN一次前向和反向传播的总通信量为4bsh， 多头注意力层一次前向和反向传播的总通信量为4bsh
4. 流水线并行，假设模型被切分到N个GPU上，一次前向和反向传播的总通信量为 2（N-1）bsh
5. ZeRO：ZeRO-1单卡通信量为2$\Phi$,ZeRO-2单卡通信量为2$\Phi$,ZeRO-3单卡通信量为3$\Phi$