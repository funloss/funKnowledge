# 1. 语义表达
## 词向量如何建模语义信息?稀疏词向量和稠密词向量有什么区别?
1. 词向量将自然语言映射到向量空间， embedding
2. 稀疏词向量：one-hot， 问题，彼此正交，距离的远近无法作为语义相似度的度量
3. 分布式语义假设：对人类语言的归纳偏置，『一个词的含义与其上下文具有很强的相关性』
4. 稠密词向量：融入先验知识将语义信息嵌入到低维连续空间向量中，先验知识的引入使得词语的表征具备语义聚类的特性
5. word2Vec，是一种浅层神经网络：
	1. 包括skip-gram和CBOW两种模型
	2. 提升模型训练速度的两种方法：层次化softmax，负采样
	3. 与bert的主要区别：无法解决一词多义的问题，bert的多头注意力可以捕捉到不同的信息

## 在构建词向量的过程中，怎么处理溢出词表词问题?
1. OOV（out of vocabulary），影响：模型在下游任务性能变差，弱化了模型泛化能力，限制了模型应用范围
2. 解法：
	1. 丢字母，改写，提取词干等查找
	2. 构建向量力度更小的子词向量
	3. 字符级别的n-gram向量（FastText）

## 词/子词/字符粒度的分词方法对构建词向量有何影响?
4. 词，英文的空格分割，不如子词稠密（do，doing完全不同，冷门词学习不充分）
5. 子词，粒度可以控制词根，词缀等，可以捕捉单词的内部结构（词性，时态，语法功能
	1. wordPiece，词片分词；BPE，字节对编码：都是将小的字符组成的基本词表不断扩充为短词组成的词元列表
	2.  对于子词合并的策略，BPE是不断和并出现频率最高的相邻子词，wordPiece执行合并时目标是最大化语料库的似然值
6. 字符，char，可以处理任何语言，但是信息过于细微，中文还好，对于英文单独的字母没有太多的意义

## 如何利用词向量进行无监督句子相似度计算任务?
1. 直接词向量加权平均作为句子向量，快速简单，但是不考虑重要性和语序，长文本效果也会很差
2. 词向量使用tf-idf作为权重来加权计算
3. 词移距离（Word Mover Distance），最小代价最大流(MCMF)求解，复杂度 $O(p^3 log(p))$

## 如何使用 BERT 构建有聚类性质的句子向量?
1. 句子向量的表征结果，需要考虑两个方面： 对齐性（alignment）语义相近的句子应该尽可能相近；均匀性（uniformity），语义不相近的句子的表征在超空间中分布均匀
2. 解法：引入对比学习来增强BERT的语义感知能力
3. 代表模型：Sentence-Bert， SimCSE

## 基于 Transformer 的预训练语言模型如何区分文本位置?
1. 引入位置编码
2. 绝对位置编码，transformer使用的sinusoidal
3. 相对位置编码，RoPE ALiBi等

## 为什么在 BERT 的输人层中3种嵌入表达要相加?
1. BERT中，词元嵌入，位置嵌入，分段嵌入相加，是一种特征交叉的做法
2. 对比concat，可以保持结构的一致性，可以提升性能，也是强制的显性特征交叉
3. 有工作指出，位置和词元的语义关联并不强，可能拆出去效果更好

## 大模型的隐含语义是如何建模的?有哪几种典型架构?
1. 大模型通过零样本推断的能力，证明了大模型可以直接将隐含的语义直接建模到完整的基于Transformer的语言模型内部，并不需要显式的二次下游的微调
2. 典型架构：纯编码器（生成能力弱），编码器-解码器架构（编码双向，解码单向），纯解码器架构（单向注意力，使用最多）

# 2. 大模型的数据
## 用来训练大模型的开源数据集有哪些?
1. 维基百科，图书，学术论文，网络爬虫，代码

## 主流开源大模型所用的训练数据量如何?各个环节的数据量如何?
略，关注各个模型的技术报告。qwen3 使用了36t token的数据

## 大模型数据预处理流程要注意哪些核心要点?
数据的质量，典型的数据清洗方法
数据的多样性，使用额外的模型通过聚类或者监督的方式来控制和评价数据的多样性

## 大模型中的扩展法则是什么?如何推演?
1. 扩展法则（scaling law）：模型的性能提升与模型的参数量大小N，数据集大小D和训练的计算量C密切相关
2. 对于常见的纯解码器模型，基本满足 $C \approx 6ND$

## 持续预训练有什么作用?如何缓解大模型微调后的通用能力遗忘问题?
1. 持续预训练的目的是向模型注入一些领域相关知识，对训练数据的质量要求比较高
2. 灾难性遗忘：模型更加倾向于为了拟合微调数据的分布情况而提升领域内相关说法的概率，导致通用数据训练后的概率空间被破坏，形成了一个有偏的模型

## 大模型指令微调有哪些筛选数据的方法?
1. 关键挑战：如何从大量数据中选择最有助于提升模型性能的训练数据
2. 除了人工筛选，当前的自动化筛选方法大多关注数据的质量，多样性，必要性这三大特点去进行选择
3. Nuggets，提出了一种方式，可以度量一条训练数据的有用程度。如果一条数据作为one-shot，对比zero-shot提升很大，则认为这条数据对于大模型的效果提升帮助很大
4. CaR，分两个阶段，数据质量打分和数据多样性排序


# 3. 大模型的预训练
大模型训练的三个阶段：**预训练，监督微调和偏好对齐**
## 预训练和监督微调有什么区别和相同之处?
1.  对于纯解码器模型，预训练的训练任务是预测下一个token（next token prediction，NTP）。其中每个token的预测仅依赖于前面的token，通过因果注意力掩码来实现（causal attention mask）。损失函数是逐token计算的
2. 监督微调同样是token的预测任务，区别是监督微调需要回传段落级别的损失
## 大模型的涌现能力指的是什么?
1. 如果一种能力不存在于规模较小的模型当中，但是存在于规模更大的模型当中，那么该能力就是涌现的
2. 能够验证大模型涌现能力的方法：少样本提示方法，增强提示策略（cot等）。都能说明，模型规模达到一定阈值后，模型表现突然提升
3. 这种也可能是评价指标不够平滑，尚无定论
## 大模型在预训练阶段有哪些提效实验和保障稳定性的方法?
1. 模型架构优化和训练策略调整：张量并行（TP）通信量大于流水线并行（PP），以及专家并行（EP）的配置优化
2. 根据scaling law合力分配参数量和资源
3. 中断恢复，自动化指标监控，使用实验管理工具和构建流水线等
## 大模型预训练、监督微调和强化学习分别解决什么问题?有什么必要性?
1. 预训练：将语料库的知识压缩进模型参数中，解决两个关键问题：1.如何利用大规模的无标注语料进行有效训练，2.如何构建能够捕捉隐含语义的表达和计算机制。基本认知：大模型在预训练已经有了大部分的知识储备
2. 监督微调：数据质量要求更高，往往是人工撰写和标注。模型可以更好的对齐人类希望的交互格式，风格和内容。『对齐税』，指模型可能过拟合监督微调的数据
3. 可以理解成，预训练是监督微调的冷启动，监督微调是强化学习的冷启动。强化学习能够更加精细的对齐人类的期望和偏好
## 大模型训练过程中如何计算显存?优化显存占用的方法有哪些?
1. 深度神经网络训练的显存消耗主要是两部分：1.模型权重训练占用的优化器状态，模型权重以及梯度。2. 模型各个非线性模块的激活值

设模型参数量为  $\Phi$

模型本身的显存占用和模型量化程度的关系：

| 量化程度      | 显存占用     |
| --------- | -------- |
| FP32      | $4\Phi$  |
| FP16/BF16 | $2\Phi$  |
| INT8      | $1\Phi$  |
| INT4      | $<1\Phi$ |
模型在非训练状态下的显存占用计算公式：

$$
\Phi = n_{vocab} \times d_{hidden}  + n_{layer} \times [d_{hidden} + (2 + 2 \frac{n_{kv\_head}}{n_{head}})d^2_{hidden} + d_{hidden} + 3\times d_{hidden}d_{FFN}] + d_{hidden} +  d_{hidden}\times n_{vocab} 
$$
训练过程中，考虑到AdamW优化器状态的参数量，和FP32训练，相关的显存状态为
$$M_{Model} = 4 \Phi $$
$$ M_{grad} = 4 \Phi $$
$$ M_{optim} = M_{momentum,FP32} + M_{variance,FP32}= 4 \Phi + 4 \Phi = 8 \Phi $$
$$M{total} = 16\Phi$$

开启混合精度训练后，优化器需要额外存储一份FP32精度的模型权重副本来稳定训练，参数量变为

$$M_{Model} = 2 \Phi $$
$$ M_{grad} = 2 \Phi $$
$$ M_{optim} = M_{model,FP32} + M_{momentum,FP32} + M_{variance,FP32}= 4 \Phi + 4 \Phi + 4 \Phi= 12 \Phi $$
$$M{total} = 16\Phi$$
可见，开启混合精度并没有减少显存占用，混合精度的有点是半精度计算加速了模型前向传播的速度，同时降低了中间激活值的显存占用。


激活内存主要由注意力机制和FFN产生，总激活显存的计算是：
$$
M_{total,act} = n_{layer} \times [(10+4\frac{n_{kv\_head}}{n_{head}})bsd_{hidden} + 8bsd_{FFN} + 5bs^2n_{head}] + 4bsd_{hidden}
$$

大模型并行策略主要有两个——数据并行和模型并行，模型并行又分为张量并行（大模型按照矩阵分块计算）和流水线并行（按层切分计算）

流水线并行与ZeRO-1相结合，显存消耗
$$
M_{per\_gpu} = \frac{2\Phi}{D_{tp}D_{pp}} + \frac{2\Phi}{D_{tp}D_{pp}} + \frac{12\Phi}{D_{dp}D_{tp}D_{pp}}
$$

$D_{dp}D_{tp}D_{pp}$ 就是显卡总数（分别是数据，张量和流水线的并行度），所以万卡集群中优化器状态甚至可以忽略不计
## 大模型训练过程中的通信开销如何计算?
1. 概念：显存墙，通信墙
2. 集合通信原语：Broadcast，Scatter，Gather，Reduce，All-Gather，Reduce-Scatter，All-Reduce，All-to-All
3. 张量并行下，FFN一次前向和反向传播的总通信量为4bsh， 多头注意力层一次前向和反向传播的总通信量为4bsh
4. 流水线并行，假设模型被切分到N个GPU上，一次前向和反向传播的总通信量为 2（N-1）bsh
5. ZeRO：ZeRO-1单卡通信量为2$\Phi$,ZeRO-2单卡通信量为2$\Phi$,ZeRO-3单卡通信量为3$\Phi$

# 4. 大模型的对齐
对齐是指模型训练过程中，确保模型的输出与特定的目标或标准一致的过程
## 大模型对齐训练需要什么样的数据?如何高效地构造这些数据?
奖励模型的数据构建：
- pointwise：对每一个提示词+回复，给一个特定的分数
- listwise：标注提示词+回复从好到坏的列表顺序
- pairwise：在两个样本对之间比较
即使是模型微调，也建议数据量大于1w，偏好对齐则需要10w条以上。数据的质量远比数据的数量重要

## 什么是 PPO?它有什么特点?
PPO是强化学习Actor-Critic分支的一种经过演进的算法，属于Policy-based系列

谱系：
- Model based：构建和利用环境模型以预测未来的状态和奖励
- Model free：不依赖环境模型，直接从与环境的交互中学习策略和价值函数
	- Value based：学习价值函数
	- Policy based：直接学习参数化的策略网络

PPO的四个核心模型：
- actor model：来源于提前准备好的监督模型，是我们强化学习的主要目标和最终输出
- critic model：来源于先前训练好的奖励模型，模型参数参与反向传播，用来预测生成回复的未来累积奖励
- reference model：模型来源于RLHF过程中，第一步监督微调模型的备份参数，训练过程中不会发生变化，作用是帮助actor model在训练中避免极端的变化
- reward model：来源于RLHF第一步提前训练好的奖励模型，训练过程中不会发生变化，主要作用是输出奖励分数来评估回复质量的好坏

## 决定奖励模型训练质量的关键因素有哪些?
reward hacking：不符合人类的真实偏好但是经过奖励模型打分后值异常高
reward model的规模和偏好数据的数量和多样性对训练结果均有显著影响
当reward model 与actor model的规模相当时，它们的泛化能力比较接近，从而容易避免reward hacking


## 提升 PPO 训练稳定性的方法有哪些?
ppo需要四个模型同时工作，对超参数非常敏感

方法：
- 设计合理的指标对ppo的训练过程进行监控
- 对损失和梯度进行标准化的裁剪
- 改进损失函数
- 优化critic model和actor model的初始化方式

## DPO算法主要解决什么问题?具体的理论依据和实现逻辑是什么?
无须借助强化学习框架，DPO可以利用偏好数据直接优化大模型以完成偏好对齐
DPO算法的目标是优化损失函数：
$$
L_{DPO}(\pi_{\theta} ; \pi_{ref}) = - \mathbb{E}_{(x,y_w,y_l)\sim D} \big[ log \sigma \big( \beta log \frac{\pi_\theta(y_w |x)}{\pi_{ref}(y_w |x)} - \beta log \frac{\pi_\theta(y_l |x)}{\pi_{ref}(y_l |x)} \big) \big]
$$
$\pi_\theta$ 是在训练过程中参数更新的模型
$\pi_{ref}$ 是参考模型，训练过程中不更新
这是一种类似交叉熵目标的做法，整个过程未使用强化学习
## 对比 DPO 和 PPO，二者各有什么特点?
- DPO所需要的训练资源比PPO少
- DPO的训练稳定性高于PPO
- PPO的泛化能力由于DPO
## 除了PPO 和 DPO，还有哪些进行偏好对齐的算法?它们各是怎样进行优化的?
PPO类：ReMax，RLOO，GRPO，REINFORCE++
DPO类：DPOP，TDPO，Self-Play Fine-Tuning
非强化学习类：Best-of-N Sampling，Rejection Sampling（拒绝采样），RAHF， Self-Rewarding
## 如何有效监控对齐训练过程中的大模型表现?
微调阶段：监督loss值，监督梯度范数
强化学习对齐阶段：监督奖励值，监督KL散度，监督困惑度或交叉熵损失，监督回复长度
## 监督微调阶段的对齐和 RLHF 阶段的对齐有何异同?
监督微调和RLHF的相似性：优化目标一致（使模型符合特定的需求和标准），训练数据形式一致（外部反馈指导模型学习）
监督微调和RLHF的差异性：sft只有预先准备好的好的数据，rlhf依赖于好和坏的比较
sft可作为RLHF的参数初始化过程，但也不是必须的（deepseek）

# 5. 大模型的垂类微调
## 在进行垂类下游任务微调时，通常是选择基座模型还是聊天模型？
基座模型：仅经过大量语料预训练得到的模型，模型更倾向于执行针对输入的续写任务，而不是专门的对话任务
聊天模型：经过了SFT和RLHF之后的模型，保留了通用语言能力且经过大量指令性数据训练
垂类任务可以使用基座模型，在保障数据质量和多样性的情况下，进行SFT

## 对大模型进行词表扩充是否有必要?它对模型的训练效果有什么影响?可以用哪些方法评估词表的效率?

有必要，尤其是跨语种训练
但是词表不是越大越好，过大信息量会变得很稀疏
压缩率：大模型在语料库上分词得到的input_ids的长度与原始文本字数的比例。压缩率越低，大模型的解码效率越高

## 提升大模型长度外推性能的方法有哪些?
外推性：指模型无须额外训练，就能将较短序列的训练效果直接应用于较长的输入序列，并保持稳定性
问题：推理时遇见训练时没有训练过的位置编码，推理时token增加导致注意力分散
解法分类：推理时优化；训练时优化
ALiBi：训练前修改长度外推，在注意力的计算的时候减去了位置的相对距离
YaRN：推理时，针对不同的情况设计分段函数

## 大模型微调时可以使用哪些损失函数，它们的原理是什么?有何特点?
- 交叉熵损失
- z-loss：使模型预测时的最大值Z倾向于维持在1附近
- EMO loss : 解决交叉熵的『硬惩罚』特性，切换为词元表征相似度的输运距离惩罚

## 如果希望将大模型用于知识密集型场景问答，并且这些场景中的知识可能会发生频繁的更新，那么在这种情况下有哪些解决方案?
- 继续预训练CT和SFT
- 检索增强生成RAG
# 6.  大模型的组件
## Transfommer 的结构和工作原理是什么?
编码器-解码器架构，各6层

编码器运算流程：
- 构建输入：词向量+编码向量
- 多头注意力计算：构建qkv向量；缩放并计算注意力分数；构建token的向量表示；计算并行化;多头注意力
- 经过残差链接和层归一化操作：
- 经过FFN

解码器的不同：
- 输入需要遮蔽当前时间步之后的信息
- 计算注意力的kv矩阵来自编码器的输出
- 输出端多了全连接层和softmax计算
## 在 Transformer 中计算注意力分数时为什么需要除以常数项?
防止点积计算出现值较大的元素，在softmax中，过大的元素会导致梯度过小，出现梯度消失现象
除以 $\sqrt{d_k}$ 后，qk乘积向量，方差会变为1

## 现有的词元化算法都有哪些?它们有何特点?
分为基于单词，字符，子词的方法
子词：BPE ，BBPE， WordPiece， ULM

## RoPE 的工作原理是什么?
旋转位置编码，对q k分别乘上旋转位置编码矩阵，再计算注意力分数
$$
\tilde{Q}(m) = R_mQ(m) \ 
\tilde{K}(n) = R_nK(n)
$$
## ALiBi的工作原理是什么?它的外推能力如何?
也是一种位置编码，整合了注意力机制和显性偏置帮助模型学习位置信息
计算方法：
$$
softmax(q_i \times K^T + m \cdot [-(i-1),...,-2,-1,0])
$$
- ALiBi的外推能力比sinusoidal，RoPE，以及T5 bias等方法更好
- ALiBi能有效利用训练长度之内的文本信息，但在利用训练长度之外的文本信息能力有限
- 无须额外的位置编码参数矩阵，计算复杂度也较低，所以性能，推理速度和显存占用都有优势

## Sparse Aueniion 是什么?有何特点?
通过稀疏化来降低Self-Attention的复杂度
Sparse Aueniion是一种优化技术，核心假设是：序列中每个元素只跟一部分其他元素相关联，而不是全部元素

膨胀自注意力机制：每个元素只跟k，2k，3k..的元素相关，其他为0。k是超参
局部自注意力机制：每个元素只跟k范围内的元素相关，其他为0。k是超参
混合稀疏自注意力机制：上面两者融合，实现长短兼顾

## Linear Attention 是什么?有何特点?
将Attention的计算复杂度从平方级降低到线性级
常见的方法有激活函数法和softmax变换法
复杂度可以从n方降低到nd
对精度会有影响

## 多头注意力机制如何用代码实现?多查询注意力和分组查询注意力的工作原理是什么?它们有何特点?

多头注意力的计算，需要三个信息
- token的q向量
- 上文的k和v向量
其中k和v是可以缓存的，因此叫kv cache
但是模型参数量大或序列很长时，kvcache占用的显存会随序列长度和模型层数线性增长。MQA和GQA都可以减少kv cache缓存的大小来提升性能

MQA是G=1时的GQA

## BatchNorm、LayerNorm 和 RMSNorm 的工作原理各是什么?三者有何特点?

归一化的作用：提升训练效率，保持梯度稳定，减轻内部协变量偏移问题
BatchNorm：对整个批次下，针对每一维特征进行归一化；推理时需要保留训练时的mini batch的移动平均值和方差
LayerNorm：在一条样本的特征维度进行归一化，推理时不需要额外的数据
RMSNorm：主流大模型多使用，在LayerNorm的基础上，保留了方差缩放，舍弃了平移操作
## PostNomm 和 PreNorm 的联系和区别分别是什么?
postNorm：
$$x_{n+1} = Norm(x_n + F(x_n)) $$
preNorm:
$$x_{n+1}  = x_n + F(Norm(x_n))$$
preNorm更加容易训练，但是模型最终性能不如postNorm。更加适合层数多的大模型，目前是主流
postNorm削弱了残差连接的权重，训练难度更高，需要配合预热等操作来稳定训练

## Dropout 的工作原理是什么?怎样避免训练推理的期望和方差出现偏移?
通过随机丢弃减少过拟合。可能导致期望偏移问题，在训练或推理中增加缩放因子解决。可能引起方差偏移问题（因此回归任务一般不使用dropout），可以使用AlphaDropout来解决

## 初始化模型参数的常见方法有哪些?它们有何特点?
- 固定值初始化：不够灵活
- 预训练初始化
- 基于固定方差的初始化
- 基于方差缩放的初始化：Xavier初始化，Kaiming初始化


# 7. 大模型的评估
## 主流的大模型评测排行榜有哪些?具体的评测形式如何?现阶段的大模型评测排行榜存在哪些问题?
## 大模型评测要关注哪些原则?
## 大模型如何修复 badcase?
## 生成式任务的经典指标有哪些?如何计算?
## 如何利用自动化测试工具评估大模型的性能?
## 如何设计大模型的对抗性测试来确保其稳健性?
## 大模型风控合规和安全考量在开发中要如何实践?大模型备案有哪些流程，需要哪些资料?

# 8. 大模型的架构
## 主流的大模型评测排行榜有哪些?具体的评测形式如何?现阶段的大模型评测排行榜存在哪些问题?
## 大模型评测要关注哪些原则?
## 大模型如何修复 badcase?
## 生成式任务的经典指标有哪些?如何计算?
## 如何利用自动化测试工具评估大模型的性能?
## 如何设计大模型的对抗性测试来确保其稳健性?
## 大模型风控合规和安全考量在开发中要如何实践?大模型备案有哪些流程，需要哪些资料?

# 9. 检索增强生成
## 大模型中的 RAG链路有哪些基本模块?如何评估各个模块的效果?
## RAG 中的召回方法有哪些?
## RAG 在召回后、生成前阶段都做了哪些工作?
## 在 RAG 工程化阶段可能会遇到哪些问题?

# 10. 大模型智能体
## 大模型智能体由哪些基本模块构成?
## 大模型智能体的规划能力有哪些提升方法?
## 大模型智能体的记忆模块在哪些方面可以优化?
## 大模型智能体的工具调用能力是什么? To0ILLM 有哪些针对性的提升点?
## XAgent框架的基本原理是什么?
## AutoGen 框架的基本原理和特点是什么?
## 结合使用 GPT-4和代码解释器，构造一个交互式编写代码的示例程序(demo)，完成从百度首页下载logo 的任务

# 11. 大模型 PEFT
## LoRA 的原理是什么?它的具体实现流程可以分为几步?
## 除了LORA，你还知道NLP任务中的哪些 PEFT方法?
## PEFT 与全参数微调该如何选型?

# 12  大模型的训练与推理
## 生成式模型的解码与采样方法有哪些?
## 大模型生成函数 generate 中各个超参数的含义及其作用是什么?
## FlashAttention 的优化方法有哪些?它如何实现数学等价性?
## MoE 并行训练中的专家并行是什么?
## vLLM 是什么?其背后的 PagedAttention 原理是什么?
## 为什么有些框架在直接使用 PyTorch 运行量化模型时速度会变得更慢?
## 数据并行、张量并行和流水线并行的工作原理分别是什么?它们的最佳组合有哪
## 些?
# 13 DeepSeek
## DeepSeek 系列大语言模型在模型架构上的创新都有哪些?
## DeepSeek-R1 的训练流程是怎样的?