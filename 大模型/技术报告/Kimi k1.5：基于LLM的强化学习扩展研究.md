https://github.com/MoonshotAI/Kimi-k1.5/blob/main/README.md
https://zhuanlan.zhihu.com/p/19832470405

Kimi K1.5采用了长文本上下文扩展和改进的策略优化方法作为核心技术，建立了一个简洁而高效的强化学习（RL）框架，==无需依赖蒙特卡罗树搜索（MCST）、价值函数和过程奖励模型（PRM）==等复杂技术。

此外，研究团队==提出的long2short方法创新性地运用long-CoT（长思维链）技术来优化short-CoT（短思维链）模型==，在多个测试中取得了突破性成果

# 引言
在Scaling Law的研究背景下，基于next token预测任务的语言模型预训练表明，==通过同比例扩展模型参数和数据规模可以持续提升模型智能水平。然而，这种方法受限于高质量训练数据的可用量==。

本研究报告介绍了多模态LLM模型Kimi k1.5的训练方案，==该模型采用RL方法训练。研究目标是探索可持续扩展的新维度。通过将RL应用于LLM，模型可以通过奖励机制进行主动探索学习，突破了静态数据集的限制==。

k1.5的设计和训练包含以下关键要素：

- ==长上下文扩展==：将RL的上下文窗口扩展至128k，研究发现随着上下文长度增加，模型性能持续提升。核心创新在于采用部分序列复用技术提升训练效率——通过重用已有序列的大部分内容来生成新序列，避免了完全重新生成的计算开销。研究表明，==上下文长度是LLM强化学习持续扩展的关键维度。==
- ==优化策略改进：==基于long-CoT提出了新的RL算法框架，并采用==online mirror descent变体实现稳健的策略优化==。通过引入高效的采样策略、长度控制机制和数据处理方案，进一步提升了算法性能。
- ==简化框架设计==：结合长上下文扩展和改进的策略优化方法，构建了一个简洁高效的LLM强化学习框架。通过扩展上下文长度，训练得到的CoT展现出规划、反思和纠正能力。增加的上下文长度相当于增加了搜索步骤数，实验证明无需借助蒙特卡罗树搜索、价值函数和过程奖励模型等复杂技术也能实现卓越性能。
- ==多模态能力==：模型通过文本和视觉数据的联合训练，获得了跨模态推理能力。
此外，研究团队提出了创新的==long2short方法==，利用long-CoT技术优化short-CoT模型，具体包括应用long-CoT激活的长度控制和模型融合技术。

 # 方法：基于LLM的强化学习
Kimi k1.5的开发经历了多个阶段：==预训练、基础监督微调（SFT）、long-CoT SFT和RL。==

本章主要讨论RL相关内容，首先概述RL提示词集合的构建和long-CoT SFT，然后再深入探讨RL训练策略，最后介绍预训练和基础监督微调的详细内容。

## RL提示词集合构建
初步实验表明，==RL提示词集合的质量和多样性对强化学习的有效性至关重要==。精心构建的提示词集合不仅能引导模型进行可靠推理，还能有效降低奖励黑客（reward hacking）风险和避免过度拟合表面特征。

高质量RL提示词集合需具备三个关键特性：

- ==多样性覆盖==：提示词应涵盖STEM、编程和通用推理等多个领域，以提升模型的适应能力并确保其在不同领域的广泛应用。
- ==难度均衡==：提示词集合应包含难度分布合理的简单、中等和困难问题，促进模型的渐进学习，避免对特定难度水平的过度拟合。
- ==评估准确性==：提示词应便于验证器进行客观可靠的评估，确保模型性能评估基于正确的推理过程，而非表面特征或随机猜测。
为实现提示词集合的多样性覆盖，研究采用自动筛选机制选择需要复杂推理且易于评估的问题。数据集包含STEM领域、竞赛题目和通用推理任务等各类问题，同时整合了纯文本和图文混合的问答数据。研究团队开发了专门的标签系统，按领域和学科对提示词进行分类，确保各学科领域的均衡分布。

研究采用基于模型的方法，==利用模型自身能力动态评估提示词难度==。具体而言，对每个提示词，==SFT模型在较高采样温度下生成十次答案，将答案正确率作为难度评估指标——正确率越低，难度越高==。这种方法使难度评估与模型的内在能力保持一致，显著提升了RL训练效果。基于此方法，研究能够有效过滤大量简单样本，并在RL训练中灵活探索不同采样策略。

为防止reward hacking，研究确保每个提示词的推理过程和最终答案都能被准确验证。实践发现，部分复杂推理问题可能存在相对简单且易于猜测的答案，导致错误验证——即模型通过错误推理得到正确答案。为此，==研究排除了选择题、判断题和证明题等容易产生此类问题的题型。==

对于通用问答任务，研究提出了一种简单有效的方法识别和剔除易于作弊的提示词：==让模型在不进行CoT推理的情况下直接猜测答案==，如果在 $N$ 次尝试内得到正确答案，则将该提示词判定为易于作弊并予以剔除。实验表明，设置 $N=8$ 可有效剔除大多数易于作弊的提示词。开发更先进的验证模型仍是未来研究的重要方向。

## 长思维链监督微调（Long-CoT SFT）
基于优化后的RL提示词集合，研究团队运用提示词工程技术构建了一个==规模精简但质量优异的long-CoT训练初始数据集，==其中包含经过严格验证的文本和图像输入推理路径。==这种方法虽与拒绝采样（RS）相似，但重点在于通过提示词工程生成long-CoT推理路径。==

训练初始数据集设计融入了人类推理的核心认知要素：

- 规划能力：模型在执行前进行系统化步骤设计
- 评估机制：对中间步骤进行严谨的分析评估
- 反思能力：支持模型对方法进行审视和优化
- 探索思维：促进模型考虑多元解决方案
通过对该训练初始数据集进行轻量级SFT，模型成功掌握了这些推理策略。经过微调的long-CoT模型在生成详实且逻辑严密的响应方面表现出显著提升，从而增强了其在多样化推理任务中的整体性能。

# 强化学习

todo


# Long2short：面向Short-CoT模型的上下文压缩
尽管long-CoT模型展现出优异性能，但相比标准short-CoT LLM，==其在测试阶段消耗更多token==。研究发现，可以将long-CoT模型的推理模式迁移至short-CoT模型，在token预算有限的约束下提升模型性能。研究提出多种解决long2short问题的方法，包括==模型合并、最短拒绝采样、DPO和long2short RL。==具体实现如下：

==模型合并== 研究发现模型合并不仅有助于保持泛化能力，在提升token效率方面也表现出效果。该方法通过==简单平均long-cot与short-cot模型的权重==，无需额外训练即可获得性能提升。

==最短拒绝采样== 基于模型对同一问题生成的响应长度具有较大变异性的观察，研究设计了最短拒绝采样方法。该方法对每个问题进行 $n$ 次采样（实验中 $n=8$ ），==选取最短的正确响应用于监督微调==。

==DPO== 采用与最短拒绝采样相似的思路，==研究使用Long-CoT模型生成多个响应样本。从中选取最短正确解答作为正样本==，将其他较长响应（包括错误答案和长度超过正样本1.5倍的正确答案）作为负样本。这些正负样本对构成对偏好数据，用于DPO训练。

==Long2short RL== 在完成标准RL训练后，研究选择一个在性能和token效率之间达到最佳平衡的模型作为基础模型，进行专门的long2short RL训练。==第二阶段引入长度惩罚机制==，显著降低最大展开长度，以加强对超长响应的惩罚（即使答案正确）。

# 训练细节补充
## 预训练阶段

Kimi k1.5基础模型采用多样化、高质量的多模态语料库进行训练。语言数据涵盖英语、中文、代码、数学推理和知识五大领域。==多模态训练数据包括图像描述、图文交错、OCR、知识库和QA数据集，构建模型的视觉-语言能力==。研究采用严格的质量控制确保预训练数据集在相关性、多样性和平衡性方面达到要求。

预训练过程分为三个阶段：

- 视觉-语言预训练：建立强大的语言基础模型，随后逐步整合多模态能力
- Cooldown阶段：使用精选和合成数据强化模型能力，重点关注推理和知识型任务
- 长上下文激活：将序列处理能力扩展至128K token

## 基础监督微调

研究团队构建了多领域的基础SFT数据集。对于问答、写作等非推理任务，首先通过人工标注建立种子数据集并训练种子模型。随后收集多样化提示词，利用种子模型生成多个响应，由标注人员排序并优化最优响应。对于数学和编程等推理任务，采用基于规则和奖励建模的验证方法，通过拒绝采样扩充数据集。

基础SFT数据集规模约100万文本样本，包括：

- 通用问答：50万样本
- 编程：20万样本
- 数学和科学：20万样本
- 创意写作：5千样本
- 长上下文任务：2万样本
另==构建100万文本-视觉样本，涵盖图表解释、OCR、图像对话、视觉编程、视觉推理及视觉辅助的数学科学问题==。

训练分两阶段：

- 32k token序列长度：1个epoch，学习率 $2 \times 10^{-5}$ 降至 $2 \times 10^{-6}$ 
- 128k token序列长度：1个epoch，学习率 $1 \times 10^{-5}$ 降至 $1 \times 10^{-6}$ 
- 为优化训练效率，采用多样本序列打包策略。

# RL基础设施
LLM大规模强化学习训练系统

在人工智能领域中，RL已经成为LLM的一个关键训练方法。这种方法源于其在复杂游戏领域取得的成功，如通过AlphaGo、AlphaStar和OpenAI Dota Five系统在围棋、星际争霸II和Dota 2等游戏中的应用。基于这些成功经验，Kimi k1.5系统采用了迭代同步RL框架，通过持续学习和适应来提升模型的推理能力。该系统的一项关键创新在于引入了Partial Rollout技术，用于优化复杂推理轨迹的处理。

![[Pasted image 20250309164034.png]]
图3：LLM大规模强化学习训练系统
如图3(a)所示，该RL训练系统采用迭代同步方式运行，每次迭代包含rollout阶段和训练阶段。在rollout阶段，中央主控协调的rollout工作器通过与模型交互生成轨迹，产生对各类输入的响应序列。这些轨迹随后存储在replay buffer中，通过打破时间相关性来确保训练数据集的多样性和无偏性。在训练阶段，训练工作器利用这些经验更新模型权重。通过这种循环过程，模型能够持续从行为中学习，并不断调整策略以提升性能。

中央主控作为核心协调者，负责管理rollout工作器、训练工作器、奖励模型评估与replay buffer之间的数据流动和通信。它确保系统高效运行，平衡负载并实现高效的数据处理。

训练工作器会处理这些rollout轨迹（无论是单次迭代完成还是跨多次迭代），计算梯度更新以优化模型参数并提升性能。奖励模型负责监督这一过程，评估模型输出质量并提供反馈指导训练。奖励模型的评估对于确定模型策略的有效性和优化模型性能起着关键作用。

此外，系统还集成了专门的代码执行服务，作为奖励模型的重要组成部分，用于处理代码相关问题。该服务在实际编程场景中评估模型输出，确保模型学习与实际编程挑战保持一致。通过实际代码执行来验证模型解决方案，这一反馈循环对于优化模型策略和提升其在代码相关任务中的表现至关重要。

## Long-CoT RL的部分展开（Partial Rollout）技术

该研究的核心思想之一是实现长上下文RL训练的扩展。==部分展开技术是一项重要创新==，通过管理长短轨迹的展开过程，有效解决了处理长CoT特征的挑战。该技术设定了固定的输出token预算，对每个展开轨迹的长度进行限制。

当轨迹在展开阶段超出token限制时，未完成部分会存入replay缓存并在下一轮迭代中继续处理。这确保了系统资源不会被单个长轨迹占用。由于展开工作器采用异步操作，部分工作器处理长轨迹时，其他工作器可以同时处理新的短轨迹任务。这种异步机制确保所有展开工作器持续参与训练过程，最大化计算效率，从而优化系统整体性能。

如图3(b)所示，Partial Rollout系统将长响应在多个迭代间（从iter n-m到iter n）分段处理。Replay Buffer作为中央存储机制维护这些响应片段，其中仅当前迭代（iter n）需要进行在线策略计算。先前的片段（iter n-m到n-1）可直接从缓存中重用，避免重复展开。这种分段方法显著降低计算开销：系统通过增量方式处理和存储片段，而非一次性处理整个响应，既保持了快速迭代速度，又支持更长响应的生成。在训练过程中，可以将特定片段排除在损失计算之外，进一步优化学习过程，使系统既高效又具有扩展性。

部分展开的实现还包含重复检测功能。系统能够识别生成内容中的重复序列并及时终止，在保证输出质量的同时减少不必要的计算。对检测到的重复内容施加额外惩罚，有效抑制提示词中的冗余内容生成。

## 训练和推理的混合部署

RL训练过程包含以下阶段：

- 训练阶段：首先，Megatron和vLLM在独立容器中运行，由checkpoint-engine进程封装。Megatron启动训练流程，完成后卸载GPU内存并准备向vLLM传输当前权重。
- 推理阶段：Megatron卸载后，vLLM以临时模型权重启动，并通过Mooncake更新从Megatron传输的最新权重。展开完成后，checkpoint-engine终止所有vLLM进程。
- 后续训练阶段：释放vLLM占用的内存后，Megatron重新加载内存并启动新一轮训练。

现有工作难以同时支持以下特性：

- 复杂并行策略：Megatron与vLLM可能采用不同的并行策略，使得Megatron在多节点分布的训练权重难以与vLLM共享。
- 最小化空闲GPU资源：在On-Policy RL中，SGLang和vLLM等最新工作可能在训练过程中占用GPU，导致训练GPU闲置。训练和推理共享设备将提高效率。
- 动态扩展能力：某些情况下，增加推理节点数量而保持训练过程不变可显著提升速度。该系统能够根据需要高效利用空闲GPU节点。
如图4所示，该混合部署框架基于Megatron和vLLM实现，训练到推理阶段转换时间不超过1分钟，反向转换仅需约10秒。


图4：混合部署框架
混合部署策略利用Kubernetes Sidecar容器共享所有可用GPU，将训练和推理工作负载整合到同一个pod中。主要优势包括：

实现高效的资源共享和管理，避免独立节点部署时训练节点等待推理节点而闲置
通过不同部署镜像，使训练和推理能够独立迭代以优化性能
架构支持灵活集成其他框架，不局限于vLLM
Checkpoint Engine负责管理vLLM进程生命周期，提供HTTP API以触发vLLM操作。系统采用etcd服务管理的全局元数据系统来广播操作和状态，确保整体一致性和可靠性。

由于CUDA图、NCCL缓冲区和NVIDIA驱动程序的影响，vLLM完全释放GPU内存存在挑战。为最小化对vLLM的修改，系统采用按需终止和重启策略，提高GPU利用率和容错能力。

Megatron中的工作器将checkpoint转换为共享内存中的Hugging Face格式，同时考虑流水线并行（PP）和专家并行（EP），仅保留张量并行（TP）。共享内存中的checkpoint随后分片并在全局元数据系统注册。系统通过Mooncake使用RDMA在节点间传输checkpoint，并对vLLM进行必要修改以支持权重文件加载和TP转换。

## 代码沙盒（Code Sandbox）

该研究开发了sandbox作为执行用户提交代码的安全环境，==专门针对代码执行和基准评估进行优化==。sandbox通过动态切换容器镜像，支持MultiPL-E、DMOJ Judge Server 2、Lean、Jupyter Notebook等不同场景应用。

在编码任务的RL训练中，sandbox通过提供一致且可重复的评估机制确保训练数据判断可靠性。==其反馈系统支持代码执行反馈和仓库级编辑等多阶段评估，同时保持统一上下文，确保不同编程语言间的公平基准比较。==

服务部署在Kubernetes上以实现可扩展性和弹性，并通过HTTP端点提供外部集成。Kubernetes的自动重启和滚动更新功能保证了系统可用性和容错能力。

为优化性能并支持RL环境，代码执行服务采用以下技术提升效率、速度和可靠性：

- 使用Crun：采用crun替代Docker作为容器运行时，显著降低容器启动时间；
- Cgroup重用：预创建容器使用的cgroup，解决高并发场景下cgroup频繁创建销毁造成的性能瓶颈；
- 磁盘使用优化：采用上层挂载为tmpfs的覆盖文件系统控制磁盘写入，提供固定大小的高速存储空间，适用于临时工作负载。

这些优化提升了代码执行中RL的效率，为评估RL生成代码提供稳定可靠的环境，对迭代训练和模型优化起到关键作用。

