https://zhuanlan.zhihu.com/p/14890557782
https://arxiv.org/abs/2412.19437

DeepSeek-V3 是一款性能卓越的混合专家（MoE）语言模型，整体参数规模达到 671B，其中每个 token 激活的参数量为 37B。

评估结果表明，DeepSeek-V3 在性能上超越了其他开源模型，并能够与主流闭源模型相媲美。

# 引言
近年来，LLM 经历了快速迭代和演进，逐步缩小了与通用人工智能（AGI）的差距。除了闭源模型外,开源模型阵营也在取得重大进展,包括 DeepSeek 系列、LLaMA 系列、Qwen 系列和 Mistral 系列，这些模型正在努力缩小与闭源模型的性能差距。

为了进一步突破开源模型的能力边界,研究团队开发了 DeepSeek-V3，==这是一个基于 MoE 架构的大模型，总参数量达到 671B，其中每个 token 会激活 37B 个参数。==

基于提升性能和降低成本的双重目标，在架构设计方面，D==eepSeek-V3 采用了MLA来确保推理效率，并使用 DeepSeekMoE来实现经济高效的训练==。这两种架构在 DeepSeek-V2 中已经得到验证，证实了它们能够在保持模型性能的同时实现高效的训练和推理。

除了延续这些基础架构外，研究团队还引入了两项创新策略来进一步提升模型性能。

首先，DeepSeek-V3 首创了==无辅助损失的负载均衡策略==，有效降低了负载均衡对模型性能的负面影响。另外，==DeepSeek-V3 采用了多 token 预测训练目标==，这种方法在评估基准测试中展现出了显著的性能提升。

为了提高训练效率，该研究采用了 ==FP8 混合精度==训练技术并对训练框架进行了全面优化。低精度训练作为一种高效的训练方案，其发展与硬件性能的提升密切相关。本研究首次在超大规模模型上成功验证了 FP8 混合精度训练框架的有效性。通过采用 FP8 计算和存储技术，显著提升了训练速度并降低了 GPU 内存占用。

在训练框架方面，研究团队开发的 ==DualPipe 算法实现了高效的流水线并行处理，减少了流水线停滞，并通过计算和通信并行处理的方式降低了训练过程中的通信开销==。这种优化确保了即使在模型规模进一步扩大的情况下，只要维持适当的计算通信比例，就能在不同节点间实现细粒度专家分配，同时将全节点间的通信开销降至接近于零。

此外,研究团队优化了跨节点的全节点通信内核，==充分利用了 InfiniBand(IB) 和 NVLink 的带宽性能。通过精细的内存优化，使得 DeepSeek-V3 的训练无需依赖成本高昂的张量并行技术==。

这些技术改进的综合运用实现了极高的训练效率。

在预训练阶段，DeepSeek-V3 使用了 ==14.8T 高质量且多样化的 token 进行训练==。整个预训练过程表现出了出人意料的稳定性，不仅没有出现不可恢复的损失突增，也未发生需要回滚的情况。

随后，模型进行了两个阶段的上下文长度扩展：==第一阶段将最大上下文长度提升至 32K，第二阶段进一步扩展至 128K==。

接着，研究团队对 DeepSeek-V3 基础模型进行了后训练，包==括 SFT 和 RL==，以增强模型对人类偏好的理解并进一步提升其性能。在后训练阶段，通过从 DeepSeek R1 系列模型中提取推理能力，同时精确控制模型的输出质量和长度比例。

DeepSeek-V3 在全面的基准测试评估中表现突出。尽管其训练成本较低，但综合评估结果显示，DeepSeek-V3-Base 已成为当前性能最强的开源基础模型，尤其在代码和数学领域表现卓越。其对话版本不仅超越了其他开源模型，还在多个标准和开放式基准测试中展现出与领先闭源模型（如 GPT-4o 和 Claude-3.5-Sonnet）相匹敌的性能。

值得注意的是，DeepSeek-V3 实现了极具竞争力的训练成本（详见表1），这得益于在算法、框架和硬件层面的整体优化设计。


表 1：DeepSeek-V3 的训练成本，假设 H800 的租赁价格为$2/GPU小时
在预训练阶段，每处理1T token 仅需 180K H800 GPU 小时，即在配备 2048 个 H800 GPU 的集群上仅需 3.7 天。因此，整个预训练阶段在不到两个月内完成，总计使用了 ==2664K GPU 小时。==

加上上下文长度扩展所需的 119K GPU 小时和后训练阶段的 5K GPU 小时，DeepSeek-V3 的完整训练总共消耗了 2.788M GPU 小时。按照每 GPU 小时 2 美元的 H800 GPU 租用价格计算，总训练成本仅为 557.6 万美元。需要说明的是，这些成本仅包含 DeepSeek-V3 的正式训练环节，不包括前期架构研究、算法验证和数据实验等相关支出。

==本研究的主要创新点包括：==

==架构创新==

在 DeepSeek-V2 高效架构的基础上，创新性地提出了无辅助损失的负载均衡策略，有效降低了负载均衡过程对模型性能的影响。

开发并验证了多 token 预测(MTP)训练目标，证实了其对模型性能的提升作用，该技术还可用于推测解码来加速推理过程。

==高效预训练==

开发了 FP8 混合精度训练框架，首次在超大规模模型上验证了 FP8 训练的可行性和效果。

通过算法、框架和硬件的综合优化，突破了跨节点 MoE 训练中的通信瓶颈，实现了计算与通信的高度重叠。这种优化大幅提升了训练效率，降低了训练成本，同时支持了更大规模模型的训练而无需额外开销。

仅用 2.664M H800 GPU 小时就完成了 DeepSeek-V3 在 14.8T token 上的预训练，打造出当前最强大的开源基础模型。预训练后的其他训练阶段仅需 0.1M GPU 小时。

==后训练——DeepSeek-R1 知识蒸馏==

该研究提出了一种创新的知识蒸馏方法，将思维链 (CoT) 模型（特别是 DeepSeek R1 系列）的推理能力转移到标准 LLM 中，尤其是 DeepSeek-V3。这一方法成功地将 R1 的验证和反思机制整合到 DeepSeek-V3 中，显著提升了其推理能力，同时有效控制了输出的风格和长度。

核心评估成果

知识领域评估：
- 在教育类基准测试中，DeepSeek-V3 的表现超越了所有开源模型，在 MMLU、MMLU-Pro 和 GPQA 测试中分别获得了 88.5、75.9 和 59.1 的优异成绩。这一性能水平已与领先闭源模型 GPT-4o 和 Claude-Sonnet-3.5 相当，显著缩小了开源与闭源模型的性能差距。
- 在事实性知识评测中，DeepSeek-V3 在 SimpleQA 和中文 SimpleQA 测试中都展现出领先于其他开源模型的优势。特别值得注意的是，虽然其英语事实知识（SimpleQA）略逊于 GPT-4o 和 Claude-Sonnet-3.5，但在中文事实知识（中文 SimpleQA）方面却超越了这些模型，凸显了其在中文知识领域的特殊优势。

技术能力评估：
- 在数学领域，DeepSeek-V3 在所有非 CoT 模型（包括开源和闭源）中取得了最优性能。值得注意的是，在 MATH-500 等特定测试中，其表现甚至超越了 GPT-4o，充分展示了其出色的数学推理能力。
- 在编程领域，DeepSeek-V3 在 LiveCodeBench 等编程竞赛基准测试中表现最为突出，确立了其在该领域的领先地位。在软件工程相关任务中，尽管略低于 Claude-Sonnet-3.5，但仍大幅领先于其他模型，展示了其在各类技术评测中的综合实力。


# 架构
DeepSeek-V3 的基本架构具有两个核心特征：

- ==采用 MLA 实现高效推理==
- ==使用 DeepSeekMoE 实现经济高效的训练。==
此外，该研究还开发了MTP训练目标，这一创新在评估基准测试中展现出显著的性能提升。

在其他未特别说明的架构细节方面，DeepSeek-V3 延续了 DeepSeek-V2 的设计方案。

## 基本架构
DeepSeek-V3 的基础架构建立在 Transformer 框架之上。为实现高效推理和降低训练成本，该模型采用了经 DeepSeek-V2 验证的 MLA 和 DeepSeekMoE 技术。相比 DeepSeek-V2，本研究在 DeepSeekMoE 中创新性地引入了无辅助损失负载均衡策略，有效降低了负载均衡过程对模型性能的影响。
![[Pasted image 20250309165327.png]]

### 多头潜在注意力机制

==DeepSeek-V3 在注意力机制方面采用了 MLA 架构。MLA 的核心创新在于对注意力键和值进行低秩联合压缩，以降低推理过程中的键值(KV)缓存开销：==

### DeepSeekMoE 及其无辅助损失负载均衡机制
DeepSeekMoE的基础架构： 在前馈网络(Feed-Forward Networks, FFN)部分，DeepSeek-V3 采用了 DeepSeekMoE 架构。相比传统的 MoE 架构（如 GShard），DeepSeekMoE 采用了更细粒度的专家分配机制，并创新性地将部分专家设置为共享专家。假设第 $t$ 个 token 的 FFN 输入为 $u_t$ ，其输出 $h_t^{\prime}$ 的计算过程如下：