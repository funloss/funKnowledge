![[Pasted image 20250309150856.png]]

https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf
https://zhuanlan.zhihu.com/p/19868935152
# 主要贡献
- ==首次验证了纯强化学习在 LLM 中显著增强推理能力的可行性==（DeepSeek-R1-Zero），即无需预先的 SFT 数据，仅通过 RL 即可激励模型学会长链推理和反思等能力。
- ==提出了多阶段训练策略（冷启动->RL->SFT->全场景 RL），有效兼顾准确率与可读性==，产出 DeepSeek-R1，性能比肩 OpenAI-o1-1217。
- 展示了知识蒸馏在提升小模型推理能力方面的潜力，并开源多个大小不一的蒸馏模型（1.5B~70B），为社区提供了可在低资源环境中也能获得高推理能力的模型选择。

# DeepSeek-R1-Zero

DeepSeek-R1-Zero 直接在基础模型上应用强化学习，不使用任何 SFT 数据。 为了训练 DeepSeek-R1-Zero，deepseek 采用了一种基于规则的奖励系统，该系统主要由两种奖励组成：

- ==准确率奖励：==准确率奖励模型评估响应是否正确。例如，在具有确定性结果的数学问题中，模型需要以指定的格式（box）提供最终答案，从而能够通过基于规则的验证来可靠地确认正确性。同样，对于 LeetCode 问题，可以使用编译器根据预定义的测试用例生成反馈。
- ==格式奖励:== 除了准确性奖励模型，还采用了一种格式奖励模型，要求模型将其思考过程放在 ‘’ 和 ‘’ 标签之间。
需要强调的是：deepseek 在训练 DeepSeek-R1-Zero 时没有使用==结果奖励（ORM）或者过程奖励（PRM）。==

在没有大量带「过程标签」（step-by-step annotation）的数据支撑下，模型如何知道自己的推理过程是否正确？

这里主要通过==「结果判定」==的方式：对于数学题、编程题等有客观正确答案的任务，可以把最终答案与标准结果对比给出奖励。虽没有逐步的过程标注，但最终答案正确与否足以在 RL 中当作回报（Reward）来引导模型学会更好的推理。

部分中间也会酌情使用格式奖励，用来约束模型输出思考过程，这是一种「作弊少、易维护」的思路。

## 顿悟时刻
在 DeepSeek-R1-Zero 的训练历程中，出现了一个特别引人注目的现象——“顿悟时刻”（aha moment）。如下图所示，这一关键时刻发生在模型的中间发展阶段。在这个阶段，DeepSeek-R1-Zero 通过重新审视其初始策略，学会了为问题分配更多思考时间。这一行为不仅彰显了模型推理能力的显著提升，也是强化学习如何催生意外且复杂成果的一个生动例证。

在大规模强化学习中，模型的「思考过程」会不断与最终的正确率奖励相互作用。当模型最初得出的答案并未得到较高奖励时，它会在后续的推理中「回头反省」，尝试补充或修正先前的思路，从而获得更高的奖励。随着强化学习的迭代，这种「主动回溯、推翻先前想法并重新推理」的行为逐渐巩固，便在输出中表现为所谓的「aha moment」。本质上，这是 RL 为模型「留出了」足够的思考和试错空间，当模型自行发现更优思路时，就会出现类似人类「恍然大悟」的瞬间。

这也展示了 RL 的强大潜力，它可以让模型在没有明确指导的情况下，自主学习并改进。

# DeepSeek-R1

DeepSeek-R1 使用了冷启动 + 多阶段训练的方式：

- 阶段1：使用少量高质量的 CoT 数据进行冷启动，预热模型。
- 阶段2：进行面向推理的强化学习，提升模型在推理任务上的性能。
- 阶段3：使用拒绝采样和监督微调，进一步提升模型的综合能力。
- 阶段4：再次进行强化学习，使模型在所有场景下都表现良好。

==DeepSeek-R1 使用冷启动数据的主要目的是为了解决 DeepSeek-R1-Zero 在训练早期出现的训练不稳定问题==。相比于直接在基础模型上进行 RL，==使用少量的 SFT 数据进行冷启动，可以让模型更快地进入稳定训练阶段：==

- 可读性：冷启动数据使用更易于理解的格式，输出内容更适合人类阅读，避免了 DeepSeek-R1-Zero 输出的语言混合、格式混乱等问题。
- 潜在性能：通过精心设计冷启动数据的模式，可以引导模型产生更好的推理能力。
- 稳定训练：使用 SFT 数据作为起始点，可以避免 RL 训练早期阶段的不稳定问题。

## 阶段1: 冷启动
冷启动阶段使用少量高质量的 CoT 数据对基础模型进行微调，作为 RL 训练的初始起点。==侧重点是让模型掌握基本的 CoT 推理能力，并使模型的输出更具可读性。==

为了获取这些数据，deepseek 探索了几种策略：利用长思维回答作为 few-shot 示例，直接提示模型生成包含反思和验证步骤的详细答案，以及收集 DeepSeek-R1-Zero 的输出并通过人工标注者进行细化。==最终收集了数千条冷启动数据==，用以微调 DeepSeek-V3-Base 作为 RL 训练的起点。DeepSeek-R1 创建的冷启动数据采用了一种可读模式，明确将输出格式定义为：`|special_token|<reasoning_process>|special_token<summary>`。

## 阶段2: 推理导向的强化学习
在冷启动模型的基础上进行 RL 训练，侧重点是提升模型在推理任务上的性能。在这个阶段，==会引入语言一致性奖励==，该奖励根据思维链（CoT）中目标语言单词的比例来计算，以减少推理过程中的语言混合问题。

尽管消融实验表明，==语言一致性奖励会导致模型性能略有下降，但它更符合人类的偏好，提高了内容的可读性==。最终，通过将推理任务的准确性与语言一致性奖励直接相加，形成了综合的奖励函数。随后，对微调后的模型进行了强化学习（RL）训练，直至其在推理任务上达到收敛。

## 阶段3: 拒绝采样和 SFT
==使用上一阶段的 RL 模型进行拒绝采样，生成高质量的推理和非推理数据，并用这些数据对模型进行微调==。侧重点是提升模型的综合能力，使其在写作、事实问答等多种任务上表现良好。

当 RL 训练接近收敛时，使用中间的 checkpoint 来采样监督微调（SFT）数据。与初期主要关注推理能力的冷启动数据不同，这一阶段加入了其他领域的数据，==旨在增强模型在写作、角色扮演以及其他通用任务上的表现。==具体的数据生成和模型微调步骤如下：

- 对于推理数据，构建推理 prompt，并从上述 RL 训练的 checkpoint 中进行拒绝采样，==以生成推理轨迹==。在之前的阶段，仅使用了基于规则的奖励来评估数据。然而，在这个阶段，通过添加其他数据来丰富数据集，其中部分数据使用了生成奖励模型，通过将真实值和模型预测输入 DeepSeek-V3 进行判断。同时，为了提升数据质量，过滤掉混合语言、长段落和代码块的思维链。对于每个提示，采样多个响应，并仅保留正确的响应。最终，收集了大约60万个与推理相关的训练样本。
- 对于非推理数据，如写作、问答、翻译等任务，使用 DeepSeek-V3 SFT 数据集的一部分。对于简单的 query，如“你好”，不使用思维链作为回答。经过筛选和整理，最终收集了大约20万个与推理无关的训练样本。
最终，使用大约80万个样本（60w推理+20w通用）对 DeepSeek-v3-Base 模型进行了两轮的 SFT。

## 阶段4: 所有场景下的强化学习
在上一阶段 SFT 模型的基础上进行 RL 训练，侧重点是使模型在所有场景下都能表现良好，包括推理任务和非推理任务，并且保证模型的安全性和无害性。


# 蒸馏小模型
为了获得更高效的小模型，并使其具有 DeekSeek-R1 的推理能力，直接对 Qwen 和 Llama 等开源模型进行了微调，使用的是上面 SFT DeepSeek-R1 的80万数据。研究结果表明，这种直接蒸馏方法显著提高了小模型的推理能力。在这里使用的基座模型是 Qwen2.5-Math-1.5B、Qwen2.5-Math-7B、Qwen2.5-14B、Qwen2.5-32B、Llama-3.1-8B 和 Llama-3.3-70B-Instruct。

对于蒸馏模型，只进行 SFT，不包括 RL 阶段，尽管加入 RL 可以显著提高模型性能。

==为什么在蒸馏到小模型时，直接用 RL 在小模型上训练不如先做大模型再蒸馏？==

大模型在 RL 阶段可能出现许多高阶推理模式。而==小模型因为容量和表示能力有限，很难在无监督或纯 RL 情境下学到相似水平的推理模式。==

蒸馏可将「大模型的推理轨迹」直接转移给小模型，小模型只需要模仿大模型相对完备的推理流程，可以在较小训练/推理开销下取得远胜于自身独立强化学习的效果。

在蒸馏模型的实现中，仅采用了 SFT 阶段，而未包含 RL 阶段，尽管 RL 的加入能显著提升模型性能。按照 deepseek 的说法，本工作的核心目的在于展示蒸馏技术的有效性，而将 RL 阶段的深入探索留给更广泛的研究社群去完成。

# 为什么 PRM 和 MCTS 没有成功？
论文中提到，基于过程奖励模型（PRM）和蒙特卡洛树搜索（MCTS）并不适合 LLM 的推理。

## PRM 的挑战
- 难以定义通用的、细粒度的推理步骤。
- 难以准确判断中间步骤的正确性，且自动标注方法效果不佳，人工标注又难以扩展。
- 模型化的 PRM 容易导致奖励黑客（Agent 利用奖励函数或环境中的漏洞来获取高奖励，而并未真正学习到预期行为。）行为，并且会增加额外的训练成本。
## MCTS 的挑战
- LLM 的 token 生成搜索空间巨大，远远超出棋类游戏，容易陷入局部最优解。
- 价值模型的训练非常困难，导致难以迭代提升。