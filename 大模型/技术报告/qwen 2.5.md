https://arxiv.org/abs/2412.15115
https://zhuanlan.zhihu.com/p/13936916587
# 概述

最近，团队发布了 Qwen 系列的最新版本 Qwen2.5。在开源部分，他们发布了7种不同规模的预训练和指令微调模型，包括0.5B、1.5B、3B、7B、14B、32B和72B，并提供了原始模型（bfloat16 精度）及不同精度的量化版本。特别地，旗舰模型 Qwen2.5-72B-Instruct 在性能上与现有最先进的开源模型 Llama-3-405B-Instruct（约大5倍）不相上下。此外，团队还发布了混合专家（MoE）模型——Qwen2.5-Turbo 和 Qwen2.5-Plus（Qwen2.5-Turbo 在 API 中标识为 qwen-turbo-2024-11-01，Qwen2.5-Plus 则标识为 qwen-plus-2024-xx-xx（待发布）。），它们分别与 GPT-4o-mini 和 GPT-4o 在性能上具有竞争力。

以下是 Qwen2.5 的关键特点：

- 规模更优：相比 Qwen2，Qwen2.5 除了提供0.5B、1.5B、7B和72B模型外，还重新引入了3B、14B和32B模型，这些模型在资源有限的场景下更具成本效益，且当前开源基础模型领域对此类型模型的关注较少。Qwen2.5-Turbo 和 Qwen2.5-Plus 在准确性、延迟和成本之间取得了良好平衡。
- 数据更优：预训练和后训练数据量有了显著提升。预训练数据量==从7T tokens增加至18T tokens==，重点涵盖知识、编程和数学。预训练阶段被设计为分阶段过渡，以适应不同数据集的融合。==后训练数据量达到1M示例，涵盖了SFT、直接偏好优化（DPO）和群体相对策略优化（GRPO）==等阶段。
- 使用体验更优：Qwen2 在使用中的关键限制已被克服，包括生成长度从2K tokens提升至==8K tokens==，更好地支持结构化输入和输出（如表格和JSON），以及更便捷的工具使用。同时，Qwen2.5-Turbo 支持最高1M tokens的上下文长度。


## 架构与Tokenizer

Qwen2.5系列主要包括用于开源的稠密模型，如 Qwen2.5-0.5B、1.5B、3B、7B、14B、32B 和 72B，以及用于API服务的 MoE 模型，如 Qwen2.5-Turbo 和 Qwen2.5-Plus。

下面详细介绍各模型的架构。

对于稠密模型，团队延续了 ==Qwen2 的 Transformer 解码器架构==，并在此基础上进行了优化。该架构包括以下关键组件：

- 分组查询注意力（Grouped Query Attention, ==GQA==）：用于高效地利用 KV 缓存（Ainslie 等，2023）；
- SwiGLU 激活函数：增强非线性激活（Dauphin 等，2017）；
- 旋转位置编码（RoPE）：用于编码位置信息（Su 等，2024）；
- QKV 偏置：用于提升注意力机制的表现（Su，2023）；
- RMSNorm：在预归一化后使用，以保证训练过程稳定（Jiang 等，2023b）。

在稠密模型的基础上，团队进一步扩展为 MoE 模型架构。通过将==标准的前馈网络（FFN）层替换为专门的 MoE 层来实现这一点==，每个层包含多个 FFN 专家，并通过路由机制将 tokens 分配给 top-K 专家。

基于 Qwen1.5-MoE 的实现，团队采用了细粒度的专家分割和共享专家路由，这些创新显著提升了模型在下游任务中的表现。

在tokenizer方面，==团队使用了 Qwen 的 tokenizer，该 tokenizer 实现了字节级别的字节对编码（BBPE）==，并采用了151,643个常规 tokens 的词汇表。与之前版本相比，他们将控制 tokens 的数量从3个扩展到了22个，新增了两个用于工具功能的 tokens，其余则用于支持其他模型能力。

这一扩展确保了所有 Qwen2.5 模型之间的统一词汇表，增强了系统的一致性并减少了潜在的兼容性问题。

# 预训练
 Qwen2.5的语言模型预训练过程包括几个关键组件。

首先，团队通过精细的过滤和评分机制，精心挑选高质量的训练数据，并结合战略性的数据混合。其次，他们对超参数进行了深入的优化研究，以确保能够有效地训练不同规模的模型。最后，他们还引入了专门的长文本预训练，增强了模型处理和理解长序列的能力。

接下来将详细介绍数据准备、超参数选择和长文本训练的具体方法。

## 预训练数据

与前代模型 Qwen2 相比，Qwen2.5 在预训练数据质量上有了显著提升。这些提升来自以下几个方面：

- 更精细的数据过滤：高质量的预训练数据对模型性能至关重要，因此数据质量评估和过滤是流程中的关键环节。团队利用 ==Qwen2-Instruct 模型作为数据质量过滤器==，进行全面的多维度分析，评估和打分训练样本。与 Qwen2 相比，这一方法显著提升了数据质量评估的能力，使得他们能够更好地保留高质量的训练数据，并有效过滤低质量的样本。
- 更优的数学与代码数据：在 Qwen2.5 的预训练过程中，==团队加入了来自 Qwen2.5-Math 和 Qwen2.5-Coder 的训练数据。==这种整合策略非常有效，因为这些专业数据集帮助他们在数学推理和代码生成任务上取得了领先的表现。
- 更高质量的合成数据：为了生成高质量的合成数据，尤其是在数学、编程和知识领域，团队采用了 Qwen2-72B-Instruct 和 Qwen2-Math-72B-Instruct。通过使用专有奖励模型和 Qwen2-Math-RM-72B 模型进行严格的过滤，进一步提高了这些合成数据的质量。
- 更合理的数据混合：为了优化预训练数据的分布，团队使用 Qwen2-Instruct 模型对不同领域的内容进行分类与平衡。分析显示，像电子商务、社交媒体和娱乐等领域在互联网数据中占比过大，常包含重复、模板化或机器生成的内容。相比之下，技术、科学和学术研究等领域虽然包含更高质量的信息，却常常被低估。通过==对过度代表的领域进行下采样，并对高价值领域进行上采样==，他们确保了一个更加平衡且信息丰富的训练数据集，更好地服务于模型的学习目标。
- 凭借这些技术手段，团队开发了一个更大、更高质量的预训练数据集，==将 Qwen2 的 7T tokens 扩展到了 18T tokens。==

## 超参数的 Scaling Law
团队基于 Qwen2.5 的预训练数据，开发了适用于超参数的 Scaling Law。虽然先前的研究主要使用 Scaling Law 来根据计算预算确定最优模型规模，但他们则利用这些 Scaling Law 来识别不同模型架构下的最优超参数。

具体来说，团队的 Scaling Law 帮助他们确定密集模型和 MoE 模型（不同规模）中的关键训练参数，如批次大小 B
 和学习率 $\mu$。

通过广泛的实验，团队系统研究了模型架构与最优训练超参数之间的关系。

具体而言，他们分析了最优学习率 和批次大小 如何随着模型规模 和预训练数据量 的变化而变化。他们的实验涵盖了多个架构，包括从 44M 到 14B 参数的密集模型，以及从 44M~1B 激活参数的 MoE 模型，训练数据集的规模从 0.8B~600B tokens 不等。

利用这些最优超参数预测，团队将最终损失率建模为模型架构和训练数据规模的函数。

此外，他们还利用 Scaling Law 来预测并比较不同参数规模的 MoE 模型与其密集模型的性能差异。通过这种分析，团队为 MoE 模型的超参数配置提供了指导，使得经过精细调整激活参数和总参数后，MoE 模型在性能上能够与特定的密集模型变种（例如 Qwen2.5-72B 和 Qwen2.5-14B）达到平衡。

## 长上下文预训练
为了最大化训练效率，Qwen2.5采用了两阶段的预训练方法：

首先使==用4K token的上下文长度进行训练==，接着进入扩展阶段，支持更长的序列。延续Qwen2的策略，在最终的预训练阶段，除Qwen2.5-Turbo外，所==有模型变体将上下文长度从4K扩展到32K token。同时，利用ABF技术将RoPE的基础频率从10,000提升到1,000,000。==

对于Qwen2.5-Turbo模型，团队实施了一个逐步扩展上下文长度的策略，经过四个阶段：==32K token、64K token、128K token，最终达到256K token，RoPE的基础频率为10,000,000。==在每个阶段，训练数据包含40%的当前最大长度序列和60%的较短序列。

这种渐进式的训练方法帮助模型平稳适应逐渐增加的上下文长度，同时保持其处理和泛化不同长度序列的能力。

为了提升模型在推理时处理更长序列的能力，团队引入了两项关键技术：==YARN和双块注意力（DCA）。==

通过这些创新，他们使得序列长度的处理能力提高了4倍，从而使Qwen2.5-Turbo能够处理最多1M个token，其他模型能够处理最多128K token。

值得一提的是，这些技术不仅通过降低困惑度改善长序列的建模效果，同时也确保了模型在处理短序列时的优异表现，从而保证在不同输入长度下提供一致的高质量输出。


# 后训练
Qwen2.5在后训练设计上相比Qwen2做出了两项显著的改进：

- 扩展的监督微调数据覆盖
Qwen2.5的监督微调过程采用了一个包含数百万个高质量示例的庞大数据集。此次数据扩展专门解决了Qwen2模型在多个关键领域中的局限性，特别是在长序列生成、数学问题解决、编码、指令执行、结构化数据理解、逻辑推理、跨语言迁移和系统指令的鲁棒性方面。

- 两阶段强化学习
Qwen2.5的强化学习过程分为两个独立的阶段：离线强化学习和在线强化学习。

- 离线强化学习：此阶段着重于==开发奖励模型难以评估的能力，==例如推理、事实准确性和指令执行。通过精心设计和验证训练数据，确保离线强化学习信号既可以学习，又具有可靠性，帮助模型有效掌握这些复杂技能。
- 在线强化学习：在线强化学习阶段==利用奖励模型检测输出质量的细微差异，==包括真实性、帮助性、简洁性、相关性、安全性以及去偏见。通过这一过程，模型能够生成精确、一致、结构清晰的回答，同时保持安全性和可读性。因此，模型的输出始终符合人类的质量标准和预期。
## 监督微调
这一部分介绍了Qwen2.5在SFT阶段的关键提升，重点在于以下几个重要领域：

- 长序列生成：Qwen2.5具备生成高质量长文本的能力，最大输出上下文长度可达到8K token，这一进步显著超越了传统后训练模型通常保持在2,000个token以内的输出限制。为此，团队开发了长响应数据集，并通过反向翻译技术从预训练语料中生成长文本数据的查询，确保输出长度符合预期，并使用Qwen2过滤低质量的配对数据。
- 数学推理：团队引入了Qwen2.5-Math中的链式推理（CoT）数据，其中包含多个来源的数据集，包括公开数据集、K-12问题集和合成问题。为了确保推理的高质量，团队采用了拒绝采样，并结合奖励建模和带注解的答案，帮助模型生成逐步推理过程。
- 编程能力：为了提升模型的编程能力，团队整合了Qwen2.5-Coder中的指令微调数据。他们将多个特定编程语言的智能体结合在一个协作框架中，生成约40种编程语言的多样化、高质量的指令对，然后进一步扩展了数据集，通过合成来自代码问答网站的示例和从GitHub收集的算法代码片段，增加了编程指令的多样性。并利用多语言沙箱进行静态代码检查，通过自动化单元测试来验证代码的质量与正确性。
- 指令跟随：为了确保指令跟随的高质量数据，团队构建了基于代码的验证框架，模型在此过程中既生成指令也生成相应的验证代码，配合全面的单元测试进行交叉验证。通过基于执行反馈的拒绝采样方法，他们精心挑选训练数据，确保模型能够精准遵循指令。
- 结构化数据理解：团队开发了一个包含传统任务（如表格问答、事实验证、错误修正和结构理解）和涉及结构化及半结构化数据的复杂任务的数据集。通过加入CoT，大大增强了模型从结构化数据中推理的能力，从而提高了在这些多样任务中的表现。这个方法不仅扩大了数据集的覆盖范围，也加深了模型从复杂数据结构中提取有意义见解的能力。
- 逻辑推理：为了提升模型的逻辑推理能力，团队引入了来自多个领域的70,000个新查询，这些查询涵盖了多项选择题、判断题和开放性问题。模型通过演绎推理、归纳推理、类比推理等多种方式系统性地处理这些问题。通过迭代优化，他们剔除错误答案和有缺陷的推理过程，从而加强了模型的推理能力，确保其在逻辑推理任务中的高效表现。
- 跨语言迁移：为了帮助模型跨语言迁移，团队使用翻译模型将高资源语言的指令翻译为多种低资源语言，从而生成对应的响应候选，并评估这些响应与原始响应的语义对齐情况，保证了响应在不同语言之间的逻辑结构和风格一致性。
- 鲁棒系统指令：通过构建数百条通用系统提示词，团队确保了后训练阶段系统提示词的多样性和一致性，评估表明模型在不同的系统提示词下仍能保持优异的表现和较小的方差，增强了模型的鲁棒性。
- 响应过滤：团队使用多种自动化注释方法进行响应评估，包括专门的评论模型和多智能体协作评分系统。所有响应都经过严格筛选，只有被所有评分系统认为完美的响应才会被保留，从而保证了输出的高质量标准。
最终，团队构建了超过100万条SFT示例的数据集，模型在32K token长度的序列上进行两轮微调，逐渐降低学习率从 7e-6
 至 7e-7，并应用适当的正则化和梯度裁剪，确保了学习的有效性并防止了过拟合。

## 离线强化学习
与在线强化学习不同，离线强化学习可以预先准备训练信号，这对于那些标准答案存在但难以通过奖励模型评估的任务尤其有利。

在本研究中，团队主要关注一些目标明确的查询领域，例如数学、编程、指令跟随和逻辑推理等，这些领域的评估可能非常复杂。

在前一阶段，团队广泛采用了执行反馈和答案匹配等策略来确保响应质量。在当前阶段，他们继续使用这一流程，借助SFT模型对一组新查询进行重采样。通过质量检查的响应将作为正面例子，而未通过的响应则作为负面例子==，用于进行直接偏好优化**（DPO）**训练。==为了进一步提高训练信号的可靠性和准确性，他们结合了人工审核与自动化审核的双重过程。这种双重方法确保训练数据不仅可学习，而且符合人类的预期。

最终，团队构建了一个包含大约150,000个训练对的数据集，随后使用在线合并优化器对模型进行了一轮训练，学习率设置为 7e-7 。

## 在线强化学习
为了构建一个强大的奖励模型用于在线强化学习，团队制定了一系列精确的标注标准。这些标准确保模型生成的响应不仅具备高质量，而且符合伦理要求以及以用户为中心的标准。具体的标注指南如下：

- 真实性：响应必须基于真实的事实，准确反映提供的上下文和指令。模型应避免生成任何虚假或缺乏数据支持的信息。
- 帮助性：模型输出的内容应切实有用，能够有效地回答用户的查询，并提供积极、富有吸引力、教育性强且相关的内容。应严格按照指令执行，确保对用户有价值。
- 简洁性：响应应简明扼要，避免无关的冗余内容，确保清晰高效地传达信息，而不让用户被细节淹没。
- 相关性：所有回应的内容都应紧密关联于用户的查询、对话历史以及助手的当前上下文。模型应定制其输出，确保与用户的需求和期望完全一致。
- 无害性：模型必须优先保障用户安全，避免产生任何可能导致非法、不道德或有害行为的内容。始终提倡道德行为和负责任的交流。
- 去偏见：模型生成的响应必须没有偏见，涉及性别、种族、国籍或政治等各类偏见，确保公平和公正，遵循广泛接受的道德和伦理标准。

用于训练奖励模型的查询来自两个不同的数据集：一是公开的开源数据集，二是一个更具挑战性的专有查询集。响应从Qwen模型的多个检查点生成，这些模型在不同的训练阶段经过SFT、DPO和RL方法的微调。为了增加多样性，响应在不同的“温度设置”下进行采样。偏好对通过人工标注和自动化标注相结合的方式生成，DPO的训练数据也被整合到其中。

在Qwen2.5的在线强化学习框架中，采用了==群体相对策略优化==。

该策略用于奖励模型训练的查询集与RL训练阶段的查询集相同。训练过程中，查询的处理顺序依据它们的响应分数方差进行调整，方差较大的查询优先处理，以确保更有效的学习。每个查询的响应团队会采样8次。所有模型的训练使用2048的全局批次大小，每个回合包含2048个样本，每对查询和响应都作为一个样本。

## 长上下文微调
为了进一步扩展Qwen2.5-Turbo的上下文处理能力，团队在后期训练阶段引入了更长的SFT示例，使得模型能够更好地适应长文本查询中的人类偏好。

在SFT阶段，团队采用了两阶段微调方法。

第一阶段中，模型==仅使用短指令进行微调，每个指令的最大长度为32K token==。该阶段使用的数据和训练步骤与其他Qwen2.5模型相同，旨在确保模型在短任务上的强大性能。

第二阶段的微调方法则==结合了短指令（最多32K token）和长指令（最多256K token）。==这种混合微调方法能够有效提升模型在长上下文任务中的指令跟随能力，同时确保它在短任务中的表现不受影响。

在强化学习阶段，团队采用与其他Qwen2.5模型类似的训练策略，主要聚焦于短指令的训练。这一设计选择基于两个主要因素：首先，长上下文任务的强化学习训练在计算上代价较高；其次，目前还缺少足够适用于长上下文任务的奖励模型。此外，他们发现，即使只在短指令上进行强化学习训练，也能够显著提高模型在长上下文任务中的人类偏好对齐程度。

